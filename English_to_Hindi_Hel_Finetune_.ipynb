{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCo5Z9TO2FnX",
    "outputId": "c5874862-c9b0-4875-cf25-87daf77ce339"
   },
   "outputs": [],
   "source": [
    "%pip install datasets transformers sacrebleu peft loralib rouge_score evaluate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZUGAcJ9V2Lim"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishalvarma/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F4I70BmO229x",
    "outputId": "ffe88cbd-cef8-4e7b-a74d-65bc3e7896c0"
   },
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"cfilt/iitb-english-hindi\") # Returns a DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MjMEppQA2_Fc",
    "outputId": "4f5b7042-326f-43dd-aa0d-a91655948976"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1659083\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 520\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2507\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYVg87xZ3MLg",
    "outputId": "bc6dc800-b2f3-49b1-e9f3-3f37e2d2735b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'Accerciser Accessibility Explorer',\n",
       "  'hi': 'एक्सेर्साइसर पहुंचनीयता अन्वेषक'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3J3b1cn33eyF"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2h2TFml63j-Z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishalvarma/Library/Python/3.9/lib/python/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-hi\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Not26eZ635St",
    "outputId": "da1c0cd9-f7a1-4750-ada5-2652fce18649"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [12110, 2, 90, 23, 19, 8800, 61, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text = \"Hello, this is a sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BcJ0FON5APX",
    "outputId": "8d54e736-2f49-43aa-c1a5-3d9c24d5ac70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[12110, 2, 90, 23, 19, 8800, 61, 0], [239, 23, 414, 8800, 3, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text = [\"Hello, this is a sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v6POJRyM5Y28",
    "outputId": "fd8eb639-8532-4272-e4fc-1b568ad24aec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2204, 10967, 818, 2, 90, 23, 19, 44, 16, 4072, 1936, 5386, 61, 0], [26618, 16155, 346, 33383, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize using the Decoder specific toeknization rules.\n",
    "print(tokenizer(text_target = [\"Hello, this is a sentence!\", \"एक्सेर्साइसर पहुंचनीयता अन्वेषक\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WKwIM-y7GWU"
   },
   "source": [
    "#### We can see that the encoder and the decoder use the same tokenization logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HS8a4tMd7Nzi"
   },
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "source_lang = 'en'\n",
    "target_lang = 'hi'\n",
    "\n",
    "def preprocess_function(examples):\n",
    "  inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
    "  targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "  model_inputs = tokenizer(inputs, max_length = max_input_length, truncation = True)\n",
    "\n",
    "  # Setup the tokenization for targets\n",
    "  labels = tokenizer(text_target=targets, max_length = max_target_length, truncation = True)\n",
    "\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4X60z95Hhio",
    "outputId": "5670462b-6049-46f9-ef3b-2c7433ee6549"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[3872, 85, 2501, 132, 15441, 36398, 0], [32643, 28541, 36253, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1]], 'labels': [[63, 2025, 18, 16155, 346, 20311, 24, 2279, 679, 0], [26618, 16155, 346, 33383, 0]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(raw_datasets['train'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HfMB6nqaIb6b"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched = True) # Applies the function 'preprocess_function' to each split (train, test, val)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(raw_datasets[\"train\"].column_names) # Removes the columns that are not needed anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GULdHm2cIjoe",
    "outputId": "cb034f6a-b118-4b79-e6ad-ae8f63ae204d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 75856896\n",
      "all model parameters: 76381184\n",
      "percentage of trainable model parameters: 99.31%\n",
      "trainable model parameters: 589824\n",
      "all model parameters: 76971008\n",
      "percentage of trainable model parameters: 0.77%\n"
     ]
    }
   ],
   "source": [
    "# Selection of the model architecture, using the weights from the pretrained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, torch_dtype = torch.bfloat16)\n",
    "\n",
    "print(print_model_parameters(model))\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 16,\n",
    "    lora_alpha = 16,\n",
    "    target_modules = [\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config).to(device)\n",
    "print(print_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Data Collator will take the data in batches rather than the whole to pass it to the model\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer = tokenizer,\n",
    "    model = peft_model,\n",
    "    return_tensors = \"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "S34XXqzwODTC"
   },
   "outputs": [],
   "source": [
    "generation_data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer = tokenizer,\n",
    "    model = peft_model,\n",
    "    return_tensors = \"pt\",\n",
    "    pad_to_multiple_of = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pBKAYLEZMiwj"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cLG-l-XEOV3O"
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(\n",
    "    tokenized_datasets['train'],\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    collate_fn = data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "K_Ek1wqdO4ua"
   },
   "outputs": [],
   "source": [
    "validation_data = DataLoader(\n",
    "    tokenized_datasets['validation'],\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    collate_fn = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mr3y4YpwPXIi"
   },
   "outputs": [],
   "source": [
    "generation_data = DataLoader(\n",
    "    tokenized_datasets['validation'],\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    collate_fn = generation_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 5\n",
    "num_train_steps = num_train_epochs * len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "XogpNgZwcXGg"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(params = peft_model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Batch 0: Loss: 3.625\n",
      "Batch 1: Loss: 2.953125\n",
      "Batch 2: Loss: 2.484375\n",
      "Batch 3: Loss: 2.765625\n",
      "Batch 4: Loss: 2.921875\n",
      "Batch 5: Loss: 3.21875\n",
      "Batch 6: Loss: 3.40625\n",
      "Batch 7: Loss: 4.71875\n",
      "Batch 8: Loss: 4.875\n",
      "Batch 9: Loss: 3.296875\n",
      "Batch 10: Loss: 1.8984375\n",
      "Batch 11: Loss: 3.8125\n",
      "Batch 12: Loss: 2.375\n",
      "Batch 13: Loss: 5.09375\n",
      "Batch 14: Loss: 3.234375\n",
      "Batch 15: Loss: 2.3125\n",
      "Batch 16: Loss: 3.015625\n",
      "Batch 17: Loss: 4.84375\n",
      "Batch 18: Loss: 3.140625\n",
      "Batch 19: Loss: 2.375\n",
      "Batch 20: Loss: 4.25\n",
      "Batch 21: Loss: 2.328125\n",
      "Batch 22: Loss: 4.28125\n",
      "Batch 23: Loss: 4.59375\n",
      "Batch 24: Loss: 3.875\n",
      "Batch 25: Loss: 3.171875\n",
      "Batch 26: Loss: 3.171875\n",
      "Batch 27: Loss: 2.984375\n",
      "Batch 28: Loss: 1.96875\n",
      "Batch 29: Loss: 2.484375\n",
      "Batch 30: Loss: 2.6875\n",
      "Batch 31: Loss: 2.046875\n",
      "Batch 32: Loss: 3.296875\n",
      "Batch 33: Loss: 3.09375\n",
      "Batch 34: Loss: 5.21875\n",
      "Batch 35: Loss: 2.453125\n",
      "Batch 36: Loss: 3.75\n",
      "Batch 37: Loss: 4.34375\n",
      "Batch 38: Loss: 2.921875\n",
      "Batch 39: Loss: 3.4375\n",
      "Batch 40: Loss: 3.5\n",
      "Batch 41: Loss: 2.875\n",
      "Batch 42: Loss: 5.75\n",
      "Batch 43: Loss: 2.296875\n",
      "Batch 44: Loss: 4.40625\n",
      "Batch 45: Loss: 4.5\n",
      "Batch 46: Loss: 3.59375\n",
      "Batch 47: Loss: 4.4375\n",
      "Batch 48: Loss: 4.5\n",
      "Batch 49: Loss: 4.3125\n",
      "Batch 50: Loss: 2.375\n",
      "Batch 51: Loss: 5.125\n",
      "Batch 52: Loss: 3.328125\n",
      "Batch 53: Loss: 3.5625\n",
      "Batch 54: Loss: 2.15625\n",
      "Batch 55: Loss: 4.65625\n",
      "Batch 56: Loss: 3.46875\n",
      "Batch 57: Loss: 4.5\n",
      "Batch 58: Loss: 4.71875\n",
      "Batch 59: Loss: 4.0625\n",
      "Batch 60: Loss: 1.3046875\n",
      "Batch 61: Loss: 3.625\n",
      "Batch 62: Loss: 3.234375\n",
      "Batch 63: Loss: 3.8125\n",
      "Batch 64: Loss: 4.28125\n",
      "Batch 65: Loss: 4.8125\n",
      "Batch 66: Loss: 3.953125\n",
      "Batch 67: Loss: 4.0625\n",
      "Batch 68: Loss: 4.71875\n",
      "Batch 69: Loss: 6.0\n",
      "Batch 70: Loss: 4.4375\n",
      "Batch 71: Loss: 4.78125\n",
      "Batch 72: Loss: 3.3125\n",
      "Batch 73: Loss: 3.828125\n",
      "Batch 74: Loss: 3.765625\n",
      "Batch 75: Loss: 5.3125\n",
      "Batch 76: Loss: 4.1875\n",
      "Batch 77: Loss: 4.28125\n",
      "Batch 78: Loss: 4.28125\n",
      "Batch 79: Loss: 4.15625\n",
      "Batch 80: Loss: 4.75\n",
      "Batch 81: Loss: 0.5078125\n",
      "Batch 82: Loss: 2.953125\n",
      "Batch 83: Loss: 3.734375\n",
      "Batch 84: Loss: 2.921875\n",
      "Batch 85: Loss: 3.703125\n",
      "Batch 86: Loss: 4.25\n",
      "Batch 87: Loss: 3.421875\n",
      "Batch 88: Loss: 3.859375\n",
      "Batch 89: Loss: 4.03125\n",
      "Batch 90: Loss: 3.296875\n",
      "Batch 91: Loss: 3.578125\n",
      "Batch 92: Loss: 3.109375\n",
      "Batch 93: Loss: 4.46875\n",
      "Batch 94: Loss: 5.15625\n",
      "Batch 95: Loss: 1.9921875\n",
      "Batch 96: Loss: 1.25\n",
      "Batch 97: Loss: 3.625\n",
      "Batch 98: Loss: 6.09375\n",
      "Batch 99: Loss: 3.28125\n",
      "Batch 100: Loss: 2.921875\n",
      "Batch 101: Loss: 3.40625\n",
      "Batch 102: Loss: 4.09375\n",
      "Batch 103: Loss: 3.015625\n",
      "Batch 104: Loss: 4.96875\n",
      "Batch 105: Loss: 2.75\n",
      "Batch 106: Loss: 4.5\n",
      "Batch 107: Loss: 2.78125\n",
      "Batch 108: Loss: 3.59375\n",
      "Batch 109: Loss: 3.15625\n",
      "Batch 110: Loss: 4.78125\n",
      "Batch 111: Loss: 3.15625\n",
      "Batch 112: Loss: 2.78125\n",
      "Batch 113: Loss: 4.84375\n",
      "Batch 114: Loss: 4.28125\n",
      "Batch 115: Loss: 3.265625\n",
      "Batch 116: Loss: 3.03125\n",
      "Batch 117: Loss: 2.3125\n",
      "Batch 118: Loss: 1.3359375\n",
      "Batch 119: Loss: 1.453125\n",
      "Batch 120: Loss: 3.21875\n",
      "Batch 121: Loss: 3.0625\n",
      "Batch 122: Loss: 3.953125\n",
      "Batch 123: Loss: 4.875\n",
      "Batch 124: Loss: 4.84375\n",
      "Batch 125: Loss: 3.53125\n",
      "Batch 126: Loss: 4.46875\n",
      "Batch 127: Loss: 5.71875\n",
      "Batch 128: Loss: 3.71875\n",
      "Batch 129: Loss: 4.5\n",
      "Batch 130: Loss: 5.21875\n",
      "Batch 131: Loss: 5.15625\n",
      "Batch 132: Loss: 3.25\n",
      "Batch 133: Loss: 5.03125\n",
      "Batch 134: Loss: 2.046875\n",
      "Batch 135: Loss: 3.96875\n",
      "Batch 136: Loss: 4.21875\n",
      "Batch 137: Loss: 4.21875\n",
      "Batch 138: Loss: 4.5\n",
      "Batch 139: Loss: 3.421875\n",
      "Batch 140: Loss: 2.78125\n",
      "Batch 141: Loss: 2.96875\n",
      "Batch 142: Loss: 5.15625\n",
      "Batch 143: Loss: 4.3125\n",
      "Batch 144: Loss: 3.734375\n",
      "Batch 145: Loss: 3.890625\n",
      "Batch 146: Loss: 3.296875\n",
      "Batch 147: Loss: 2.640625\n",
      "Batch 148: Loss: 3.671875\n",
      "Batch 149: Loss: 4.4375\n",
      "Batch 150: Loss: 5.25\n",
      "Batch 151: Loss: 5.03125\n",
      "Batch 152: Loss: 2.9375\n",
      "Batch 153: Loss: 4.3125\n",
      "Batch 154: Loss: 4.625\n",
      "Batch 155: Loss: 3.703125\n",
      "Batch 156: Loss: 3.03125\n",
      "Batch 157: Loss: 3.609375\n",
      "Batch 158: Loss: 5.21875\n",
      "Batch 159: Loss: 4.03125\n",
      "Batch 160: Loss: 3.125\n",
      "Batch 161: Loss: 3.734375\n",
      "Batch 162: Loss: 4.25\n",
      "Batch 163: Loss: 2.484375\n",
      "Batch 164: Loss: 2.078125\n",
      "Batch 165: Loss: 3.984375\n",
      "Batch 166: Loss: 4.96875\n",
      "Batch 167: Loss: 2.265625\n",
      "Batch 168: Loss: 4.21875\n",
      "Batch 169: Loss: 3.65625\n",
      "Batch 170: Loss: 2.359375\n",
      "Batch 171: Loss: 2.359375\n",
      "Batch 172: Loss: 6.0\n",
      "Batch 173: Loss: 1.6953125\n",
      "Batch 174: Loss: 3.90625\n",
      "Batch 175: Loss: 3.65625\n",
      "Batch 176: Loss: 3.84375\n",
      "Batch 177: Loss: 2.96875\n",
      "Batch 178: Loss: 3.71875\n",
      "Batch 179: Loss: 2.359375\n",
      "Batch 180: Loss: 2.625\n",
      "Batch 181: Loss: 3.609375\n",
      "Batch 182: Loss: 4.125\n",
      "Batch 183: Loss: 1.8515625\n",
      "Batch 184: Loss: 4.0625\n",
      "Batch 185: Loss: 2.265625\n",
      "Batch 186: Loss: 4.15625\n",
      "Batch 187: Loss: 3.96875\n",
      "Batch 188: Loss: 3.0625\n",
      "Batch 189: Loss: 3.65625\n",
      "Batch 190: Loss: 3.8125\n",
      "Batch 191: Loss: 4.375\n",
      "Batch 192: Loss: 3.984375\n",
      "Batch 193: Loss: 4.65625\n",
      "Batch 194: Loss: 4.34375\n",
      "Batch 195: Loss: 2.109375\n",
      "Batch 196: Loss: 3.28125\n",
      "Batch 197: Loss: 4.125\n",
      "Batch 198: Loss: 4.71875\n",
      "Batch 199: Loss: 3.390625\n",
      "Batch 200: Loss: 3.375\n",
      "Batch 201: Loss: 3.03125\n",
      "Batch 202: Loss: 3.859375\n",
      "Batch 203: Loss: 1.8125\n",
      "Batch 204: Loss: 2.375\n",
      "Batch 205: Loss: 2.546875\n",
      "Batch 206: Loss: 4.375\n",
      "Batch 207: Loss: 4.5625\n",
      "Batch 208: Loss: 5.21875\n",
      "Batch 209: Loss: 2.359375\n",
      "Batch 210: Loss: 2.96875\n",
      "Batch 211: Loss: 4.15625\n",
      "Batch 212: Loss: 1.734375\n",
      "Batch 213: Loss: 4.40625\n",
      "Batch 214: Loss: 3.578125\n",
      "Batch 215: Loss: 3.21875\n",
      "Batch 216: Loss: 5.03125\n",
      "Batch 217: Loss: 4.9375\n",
      "Batch 218: Loss: 2.375\n",
      "Batch 219: Loss: 3.421875\n",
      "Batch 220: Loss: 3.734375\n",
      "Batch 221: Loss: 2.8125\n",
      "Batch 222: Loss: 2.40625\n",
      "Batch 223: Loss: 3.875\n",
      "Batch 224: Loss: 2.46875\n",
      "Batch 225: Loss: 1.7421875\n",
      "Batch 226: Loss: 2.109375\n",
      "Batch 227: Loss: 1.0546875\n",
      "Batch 228: Loss: 4.3125\n",
      "Batch 229: Loss: 4.96875\n",
      "Batch 230: Loss: 1.484375\n",
      "Batch 231: Loss: 3.328125\n",
      "Batch 232: Loss: 3.40625\n",
      "Batch 233: Loss: 4.4375\n",
      "Batch 234: Loss: 4.15625\n",
      "Batch 235: Loss: 3.453125\n",
      "Batch 236: Loss: 4.9375\n",
      "Batch 237: Loss: 2.859375\n",
      "Batch 238: Loss: 1.5\n",
      "Batch 239: Loss: 1.734375\n",
      "Batch 240: Loss: 3.859375\n",
      "Batch 241: Loss: 1.640625\n",
      "Batch 242: Loss: 4.4375\n",
      "Batch 243: Loss: 3.875\n",
      "Batch 244: Loss: 2.5625\n",
      "Batch 245: Loss: 1.2890625\n",
      "Batch 246: Loss: 4.625\n",
      "Batch 247: Loss: 1.5234375\n",
      "Batch 248: Loss: 1.578125\n",
      "Batch 249: Loss: 2.921875\n",
      "Batch 250: Loss: 4.03125\n",
      "Batch 251: Loss: 4.0\n",
      "Batch 252: Loss: 3.09375\n",
      "Batch 253: Loss: 1.390625\n",
      "Batch 254: Loss: 2.25\n",
      "Batch 255: Loss: 4.1875\n",
      "Batch 256: Loss: 4.09375\n",
      "Batch 257: Loss: 2.4375\n",
      "Batch 258: Loss: 3.140625\n",
      "Batch 259: Loss: 3.203125\n",
      "Batch 260: Loss: 1.7109375\n",
      "Batch 261: Loss: 4.03125\n",
      "Batch 262: Loss: 1.9375\n",
      "Batch 263: Loss: 3.90625\n",
      "Batch 264: Loss: 2.390625\n",
      "Batch 265: Loss: 5.03125\n",
      "Batch 266: Loss: 3.015625\n",
      "Batch 267: Loss: 2.78125\n",
      "Batch 268: Loss: 4.125\n",
      "Batch 269: Loss: 4.34375\n",
      "Batch 270: Loss: 3.484375\n",
      "Batch 271: Loss: 2.453125\n",
      "Batch 272: Loss: 1.6171875\n",
      "Batch 273: Loss: 3.53125\n",
      "Batch 274: Loss: 3.4375\n",
      "Batch 275: Loss: 4.5\n",
      "Batch 276: Loss: 3.953125\n",
      "Batch 277: Loss: 0.96484375\n",
      "Batch 278: Loss: 2.234375\n",
      "Batch 279: Loss: 2.828125\n",
      "Batch 280: Loss: 3.46875\n",
      "Batch 281: Loss: 6.75\n",
      "Batch 282: Loss: 2.078125\n",
      "Batch 283: Loss: 1.796875\n",
      "Batch 284: Loss: 5.125\n",
      "Batch 285: Loss: 4.28125\n",
      "Batch 286: Loss: 2.78125\n",
      "Batch 287: Loss: 3.625\n",
      "Batch 288: Loss: 5.0\n",
      "Batch 289: Loss: 2.28125\n",
      "Batch 290: Loss: 2.109375\n",
      "Batch 291: Loss: 3.046875\n",
      "Batch 292: Loss: 0.78125\n",
      "Batch 293: Loss: 3.484375\n",
      "Batch 294: Loss: 3.296875\n",
      "Batch 295: Loss: 3.125\n",
      "Batch 296: Loss: 3.21875\n",
      "Batch 297: Loss: 4.09375\n",
      "Batch 298: Loss: 4.5625\n",
      "Batch 299: Loss: 2.328125\n",
      "Batch 300: Loss: 3.171875\n",
      "Batch 301: Loss: 5.3125\n",
      "Batch 302: Loss: 0.75390625\n",
      "Batch 303: Loss: 3.5625\n",
      "Batch 304: Loss: 3.640625\n",
      "Batch 305: Loss: 4.5625\n",
      "Batch 306: Loss: 3.09375\n",
      "Batch 307: Loss: 3.703125\n",
      "Batch 308: Loss: 5.3125\n",
      "Batch 309: Loss: 3.203125\n",
      "Batch 310: Loss: 2.4375\n",
      "Batch 311: Loss: 3.171875\n",
      "Batch 312: Loss: 4.3125\n",
      "Batch 313: Loss: 3.484375\n",
      "Batch 314: Loss: 3.140625\n",
      "Batch 315: Loss: 3.984375\n",
      "Batch 316: Loss: 1.390625\n",
      "Batch 317: Loss: 1.953125\n",
      "Batch 318: Loss: 3.1875\n",
      "Batch 319: Loss: 3.875\n",
      "Batch 320: Loss: 4.09375\n",
      "Batch 321: Loss: 2.78125\n",
      "Batch 322: Loss: 3.40625\n",
      "Batch 323: Loss: 2.84375\n",
      "Batch 324: Loss: 3.734375\n",
      "Batch 325: Loss: 3.875\n",
      "Batch 326: Loss: 2.59375\n",
      "Batch 327: Loss: 2.359375\n",
      "Batch 328: Loss: 2.359375\n",
      "Batch 329: Loss: 2.28125\n",
      "Batch 330: Loss: 2.859375\n",
      "Batch 331: Loss: 2.953125\n",
      "Batch 332: Loss: 3.484375\n",
      "Batch 333: Loss: 3.640625\n",
      "Batch 334: Loss: 3.90625\n",
      "Batch 335: Loss: 3.84375\n",
      "Batch 336: Loss: 1.8984375\n",
      "Batch 337: Loss: 3.75\n",
      "Batch 338: Loss: 2.8125\n",
      "Batch 339: Loss: 1.7109375\n",
      "Batch 340: Loss: 4.0625\n",
      "Batch 341: Loss: 2.046875\n",
      "Batch 342: Loss: 1.859375\n",
      "Batch 343: Loss: 5.375\n",
      "Batch 344: Loss: 1.703125\n",
      "Batch 345: Loss: 4.0\n",
      "Batch 346: Loss: 2.3125\n",
      "Batch 347: Loss: 3.734375\n",
      "Batch 348: Loss: 1.5625\n",
      "Batch 349: Loss: 2.59375\n",
      "Batch 350: Loss: 7.03125\n",
      "Batch 351: Loss: 3.359375\n",
      "Batch 352: Loss: 1.734375\n",
      "Batch 353: Loss: 1.5546875\n",
      "Batch 354: Loss: 1.625\n",
      "Batch 355: Loss: 3.265625\n",
      "Batch 356: Loss: 1.8828125\n",
      "Batch 357: Loss: 2.890625\n",
      "Batch 358: Loss: 3.546875\n",
      "Batch 359: Loss: 4.03125\n",
      "Batch 360: Loss: 4.875\n",
      "Batch 361: Loss: 4.625\n",
      "Batch 362: Loss: 4.25\n",
      "Batch 363: Loss: 4.5\n",
      "Batch 364: Loss: 3.28125\n",
      "Batch 365: Loss: 3.59375\n",
      "Batch 366: Loss: 3.25\n",
      "Batch 367: Loss: 2.078125\n",
      "Batch 368: Loss: 2.046875\n",
      "Batch 369: Loss: 3.953125\n",
      "Batch 370: Loss: 3.375\n",
      "Batch 371: Loss: 3.921875\n",
      "Batch 372: Loss: 3.609375\n",
      "Batch 373: Loss: 1.8125\n",
      "Batch 374: Loss: 3.921875\n",
      "Batch 375: Loss: 2.84375\n",
      "Batch 376: Loss: 3.0625\n",
      "Batch 377: Loss: 3.78125\n",
      "Batch 378: Loss: 3.96875\n",
      "Batch 379: Loss: 2.359375\n",
      "Batch 380: Loss: 3.796875\n",
      "Batch 381: Loss: 3.234375\n",
      "Batch 382: Loss: 2.75\n",
      "Batch 383: Loss: 7.5\n",
      "Batch 384: Loss: 3.796875\n",
      "Batch 385: Loss: 4.125\n",
      "Batch 386: Loss: 3.65625\n",
      "Batch 387: Loss: 2.375\n",
      "Batch 388: Loss: 3.703125\n",
      "Batch 389: Loss: 2.734375\n",
      "Batch 390: Loss: 3.484375\n",
      "Batch 391: Loss: 2.46875\n",
      "Batch 392: Loss: 2.8125\n",
      "Batch 393: Loss: 3.640625\n",
      "Batch 394: Loss: 4.0625\n",
      "Batch 395: Loss: 1.6015625\n",
      "Batch 396: Loss: 3.328125\n",
      "Batch 397: Loss: 4.5\n",
      "Batch 398: Loss: 1.5234375\n",
      "Batch 399: Loss: 2.65625\n",
      "Batch 400: Loss: 3.390625\n",
      "Batch 401: Loss: 4.5\n",
      "Batch 402: Loss: 2.890625\n",
      "Batch 403: Loss: 3.59375\n",
      "Batch 404: Loss: 3.875\n",
      "Batch 405: Loss: 2.484375\n",
      "Batch 406: Loss: 3.640625\n",
      "Batch 407: Loss: 3.671875\n",
      "Batch 408: Loss: 4.59375\n",
      "Batch 409: Loss: 3.3125\n",
      "Batch 410: Loss: 4.90625\n",
      "Batch 411: Loss: 4.4375\n",
      "Batch 412: Loss: 2.78125\n",
      "Batch 413: Loss: 3.796875\n",
      "Batch 414: Loss: 2.03125\n",
      "Batch 415: Loss: 2.453125\n",
      "Batch 416: Loss: 2.90625\n",
      "Batch 417: Loss: 2.453125\n",
      "Batch 418: Loss: 5.34375\n",
      "Batch 419: Loss: 3.109375\n",
      "Batch 420: Loss: 3.203125\n",
      "Batch 421: Loss: 4.46875\n",
      "Batch 422: Loss: 1.5859375\n",
      "Batch 423: Loss: 2.34375\n",
      "Batch 424: Loss: 4.0\n",
      "Batch 425: Loss: 5.25\n",
      "Batch 426: Loss: 2.53125\n",
      "Batch 427: Loss: 3.25\n",
      "Batch 428: Loss: 3.140625\n",
      "Batch 429: Loss: 2.375\n",
      "Batch 430: Loss: 3.171875\n",
      "Batch 431: Loss: 3.90625\n",
      "Batch 432: Loss: 4.15625\n",
      "Batch 433: Loss: 1.3125\n",
      "Batch 434: Loss: 4.0625\n",
      "Batch 435: Loss: 2.203125\n",
      "Batch 436: Loss: 3.65625\n",
      "Batch 437: Loss: 3.40625\n",
      "Batch 438: Loss: 3.203125\n",
      "Batch 439: Loss: 3.203125\n",
      "Batch 440: Loss: 3.46875\n",
      "Batch 441: Loss: 4.75\n",
      "Batch 442: Loss: 1.15625\n",
      "Batch 443: Loss: 1.7265625\n",
      "Batch 444: Loss: 2.984375\n",
      "Batch 445: Loss: 3.828125\n",
      "Batch 446: Loss: 3.96875\n",
      "Batch 447: Loss: 2.828125\n",
      "Batch 448: Loss: 2.34375\n",
      "Batch 449: Loss: 4.0\n",
      "Batch 450: Loss: 1.6328125\n",
      "Batch 451: Loss: 1.015625\n",
      "Batch 452: Loss: 4.1875\n",
      "Batch 453: Loss: 1.9609375\n",
      "Batch 454: Loss: 3.296875\n",
      "Batch 455: Loss: 4.40625\n",
      "Batch 456: Loss: 0.81640625\n",
      "Batch 457: Loss: 3.859375\n",
      "Batch 458: Loss: 3.84375\n",
      "Batch 459: Loss: 3.75\n",
      "Batch 460: Loss: 3.078125\n",
      "Batch 461: Loss: 4.71875\n",
      "Batch 462: Loss: 1.625\n",
      "Batch 463: Loss: 3.84375\n",
      "Batch 464: Loss: 3.671875\n",
      "Batch 465: Loss: 2.015625\n",
      "Batch 466: Loss: 4.21875\n",
      "Batch 467: Loss: 3.046875\n",
      "Batch 468: Loss: 2.484375\n",
      "Batch 469: Loss: 3.171875\n",
      "Batch 470: Loss: 4.21875\n",
      "Batch 471: Loss: 3.625\n",
      "Batch 472: Loss: 3.71875\n",
      "Batch 473: Loss: 3.125\n",
      "Batch 474: Loss: 4.5625\n",
      "Batch 475: Loss: 3.75\n",
      "Batch 476: Loss: 2.15625\n",
      "Batch 477: Loss: 5.09375\n",
      "Batch 478: Loss: 3.46875\n",
      "Batch 479: Loss: 4.59375\n",
      "Batch 480: Loss: 4.75\n",
      "Batch 481: Loss: 3.25\n",
      "Batch 482: Loss: 1.3671875\n",
      "Batch 483: Loss: 1.9375\n",
      "Batch 484: Loss: 3.890625\n",
      "Batch 485: Loss: 4.8125\n",
      "Batch 486: Loss: 1.765625\n",
      "Batch 487: Loss: 4.5\n",
      "Batch 488: Loss: 3.734375\n",
      "Batch 489: Loss: 2.53125\n",
      "Batch 490: Loss: 1.484375\n",
      "Batch 491: Loss: 3.453125\n",
      "Batch 492: Loss: 4.25\n",
      "Batch 493: Loss: 3.375\n",
      "Batch 494: Loss: 2.984375\n",
      "Batch 495: Loss: 2.375\n",
      "Batch 496: Loss: 3.859375\n",
      "Batch 497: Loss: 1.953125\n",
      "Batch 498: Loss: 3.5625\n",
      "Batch 499: Loss: 3.34375\n",
      "Batch 500: Loss: 3.265625\n",
      "Batch 501: Loss: 4.03125\n",
      "Batch 502: Loss: 3.0625\n",
      "Batch 503: Loss: 0.412109375\n",
      "Batch 504: Loss: 4.0625\n",
      "Batch 505: Loss: 3.25\n",
      "Batch 506: Loss: 3.5625\n",
      "Batch 507: Loss: 4.0625\n",
      "Batch 508: Loss: 2.359375\n",
      "Batch 509: Loss: 4.78125\n",
      "Batch 510: Loss: 2.71875\n",
      "Batch 511: Loss: 3.875\n",
      "Batch 512: Loss: 3.84375\n",
      "Batch 513: Loss: 4.375\n",
      "Batch 514: Loss: 2.421875\n",
      "Batch 515: Loss: 2.875\n",
      "Batch 516: Loss: 4.71875\n",
      "Batch 517: Loss: 3.953125\n",
      "Batch 518: Loss: 3.78125\n",
      "Batch 519: Loss: 2.546875\n",
      "Batch 520: Loss: 2.578125\n",
      "Batch 521: Loss: 3.171875\n",
      "Batch 522: Loss: 2.5\n",
      "Batch 523: Loss: 3.625\n",
      "Batch 524: Loss: 4.0\n",
      "Batch 525: Loss: 3.5625\n",
      "Batch 526: Loss: 1.8125\n",
      "Batch 527: Loss: 4.84375\n",
      "Batch 528: Loss: 1.2578125\n",
      "Batch 529: Loss: 4.5\n",
      "Batch 530: Loss: 3.484375\n",
      "Batch 531: Loss: 3.03125\n",
      "Batch 532: Loss: 2.84375\n",
      "Batch 533: Loss: 4.15625\n",
      "Batch 534: Loss: 4.78125\n",
      "Batch 535: Loss: 3.078125\n",
      "Batch 536: Loss: 3.109375\n",
      "Batch 537: Loss: 3.1875\n",
      "Batch 538: Loss: 2.234375\n",
      "Batch 539: Loss: 3.515625\n",
      "Batch 540: Loss: 2.125\n",
      "Batch 541: Loss: 2.34375\n",
      "Batch 542: Loss: 2.703125\n",
      "Batch 543: Loss: 4.125\n",
      "Batch 544: Loss: 4.625\n",
      "Batch 545: Loss: 4.15625\n",
      "Batch 546: Loss: 4.40625\n",
      "Batch 547: Loss: 3.765625\n",
      "Batch 548: Loss: 2.375\n",
      "Batch 549: Loss: 3.578125\n",
      "Batch 550: Loss: 2.46875\n",
      "Batch 551: Loss: 2.484375\n",
      "Batch 552: Loss: 3.234375\n",
      "Batch 553: Loss: 4.84375\n",
      "Batch 554: Loss: 3.46875\n",
      "Batch 555: Loss: 3.1875\n",
      "Batch 556: Loss: 3.609375\n",
      "Batch 557: Loss: 4.90625\n",
      "Batch 558: Loss: 2.78125\n",
      "Batch 559: Loss: 2.96875\n",
      "Batch 560: Loss: 2.75\n",
      "Batch 561: Loss: 2.078125\n",
      "Batch 562: Loss: 2.71875\n",
      "Batch 563: Loss: 4.375\n",
      "Batch 564: Loss: 3.5\n",
      "Batch 565: Loss: 4.3125\n",
      "Batch 566: Loss: 2.203125\n",
      "Batch 567: Loss: 4.125\n",
      "Batch 568: Loss: 3.25\n",
      "Batch 569: Loss: 4.625\n",
      "Batch 570: Loss: 3.5\n",
      "Batch 571: Loss: 3.953125\n",
      "Batch 572: Loss: 2.375\n",
      "Batch 573: Loss: 2.109375\n",
      "Batch 574: Loss: 4.1875\n",
      "Batch 575: Loss: 3.5\n",
      "Batch 576: Loss: 4.09375\n",
      "Batch 577: Loss: 2.5\n",
      "Batch 578: Loss: 3.96875\n",
      "Batch 579: Loss: 3.203125\n",
      "Batch 580: Loss: 3.09375\n",
      "Batch 581: Loss: 2.296875\n",
      "Batch 582: Loss: 2.296875\n",
      "Batch 583: Loss: 2.65625\n",
      "Batch 584: Loss: 3.890625\n",
      "Batch 585: Loss: 3.046875\n",
      "Batch 586: Loss: 2.140625\n",
      "Batch 587: Loss: 4.8125\n",
      "Batch 588: Loss: 3.859375\n",
      "Batch 589: Loss: 4.21875\n",
      "Batch 590: Loss: 0.96875\n",
      "Batch 591: Loss: 3.03125\n",
      "Batch 592: Loss: 2.265625\n",
      "Batch 593: Loss: 4.5\n",
      "Batch 594: Loss: 3.65625\n",
      "Batch 595: Loss: 5.34375\n",
      "Batch 596: Loss: 2.015625\n",
      "Batch 597: Loss: 3.671875\n",
      "Batch 598: Loss: 4.625\n",
      "Batch 599: Loss: 4.21875\n",
      "Batch 600: Loss: 1.4296875\n",
      "Batch 601: Loss: 4.625\n",
      "Batch 602: Loss: 4.3125\n",
      "Batch 603: Loss: 2.484375\n",
      "Batch 604: Loss: 3.671875\n",
      "Batch 605: Loss: 4.28125\n",
      "Batch 606: Loss: 5.15625\n",
      "Batch 607: Loss: 4.34375\n",
      "Batch 608: Loss: 2.6875\n",
      "Batch 609: Loss: 4.46875\n",
      "Batch 610: Loss: 2.03125\n",
      "Batch 611: Loss: 4.40625\n",
      "Batch 612: Loss: 2.78125\n",
      "Batch 613: Loss: 1.9453125\n",
      "Batch 614: Loss: 4.71875\n",
      "Batch 615: Loss: 3.296875\n",
      "Batch 616: Loss: 3.234375\n",
      "Batch 617: Loss: 3.625\n",
      "Batch 618: Loss: 3.109375\n",
      "Batch 619: Loss: 2.203125\n",
      "Batch 620: Loss: 3.890625\n",
      "Batch 621: Loss: 4.0\n",
      "Batch 622: Loss: 3.234375\n",
      "Batch 623: Loss: 4.125\n",
      "Batch 624: Loss: 3.25\n",
      "Batch 625: Loss: 2.421875\n",
      "Batch 626: Loss: 3.40625\n",
      "Batch 627: Loss: 3.078125\n",
      "Batch 628: Loss: 3.28125\n",
      "Batch 629: Loss: 4.625\n",
      "Batch 630: Loss: 3.234375\n",
      "Batch 631: Loss: 4.5\n",
      "Batch 632: Loss: 3.21875\n",
      "Batch 633: Loss: 3.34375\n",
      "Batch 634: Loss: 3.140625\n",
      "Batch 635: Loss: 4.65625\n",
      "Batch 636: Loss: 1.640625\n",
      "Batch 637: Loss: 2.015625\n",
      "Batch 638: Loss: 3.984375\n",
      "Batch 639: Loss: 4.0625\n",
      "Batch 640: Loss: 4.0625\n",
      "Batch 641: Loss: 2.734375\n",
      "Batch 642: Loss: 3.421875\n",
      "Batch 643: Loss: 3.828125\n",
      "Batch 644: Loss: 3.265625\n",
      "Batch 645: Loss: 3.9375\n",
      "Batch 646: Loss: 3.078125\n",
      "Batch 647: Loss: 1.8046875\n",
      "Batch 648: Loss: 3.96875\n",
      "Batch 649: Loss: 2.359375\n",
      "Batch 650: Loss: 4.4375\n",
      "Batch 651: Loss: 2.5625\n",
      "Batch 652: Loss: 1.546875\n",
      "Batch 653: Loss: 1.9140625\n",
      "Batch 654: Loss: 2.21875\n",
      "Batch 655: Loss: 2.28125\n",
      "Batch 656: Loss: 3.53125\n",
      "Batch 657: Loss: 4.0\n",
      "Batch 658: Loss: 3.328125\n",
      "Batch 659: Loss: 3.3125\n",
      "Batch 660: Loss: 2.609375\n",
      "Batch 661: Loss: 4.1875\n",
      "Batch 662: Loss: 2.34375\n",
      "Batch 663: Loss: 3.796875\n",
      "Batch 664: Loss: 2.53125\n",
      "Batch 665: Loss: 2.609375\n",
      "Batch 666: Loss: 3.484375\n",
      "Batch 667: Loss: 4.15625\n",
      "Batch 668: Loss: 4.625\n",
      "Batch 669: Loss: 4.46875\n",
      "Batch 670: Loss: 5.0\n",
      "Batch 671: Loss: 2.84375\n",
      "Batch 672: Loss: 3.828125\n",
      "Batch 673: Loss: 3.34375\n",
      "Batch 674: Loss: 1.9375\n",
      "Batch 675: Loss: 2.625\n",
      "Batch 676: Loss: 2.25\n",
      "Batch 677: Loss: 2.375\n",
      "Batch 678: Loss: 3.25\n",
      "Batch 679: Loss: 1.53125\n",
      "Batch 680: Loss: 1.6875\n",
      "Batch 681: Loss: 4.46875\n",
      "Batch 682: Loss: 2.90625\n",
      "Batch 683: Loss: 2.359375\n",
      "Batch 684: Loss: 3.453125\n",
      "Batch 685: Loss: 3.6875\n",
      "Batch 686: Loss: 4.0\n",
      "Batch 687: Loss: 3.0625\n",
      "Batch 688: Loss: 4.5\n",
      "Batch 689: Loss: 3.171875\n",
      "Batch 690: Loss: 4.78125\n",
      "Batch 691: Loss: 2.765625\n",
      "Batch 692: Loss: 4.28125\n",
      "Batch 693: Loss: 4.1875\n",
      "Batch 694: Loss: 3.734375\n",
      "Batch 695: Loss: 0.7890625\n",
      "Batch 696: Loss: 3.171875\n",
      "Batch 697: Loss: 4.5\n",
      "Batch 698: Loss: 3.453125\n",
      "Batch 699: Loss: 2.703125\n",
      "Batch 700: Loss: 2.359375\n",
      "Batch 701: Loss: 2.546875\n",
      "Batch 702: Loss: 4.375\n",
      "Batch 703: Loss: 2.96875\n",
      "Batch 704: Loss: 4.78125\n",
      "Batch 705: Loss: 3.59375\n",
      "Batch 706: Loss: 2.140625\n",
      "Batch 707: Loss: 3.796875\n",
      "Batch 708: Loss: 3.234375\n",
      "Batch 709: Loss: 3.53125\n",
      "Batch 710: Loss: 3.859375\n",
      "Batch 711: Loss: 3.9375\n",
      "Batch 712: Loss: 3.828125\n",
      "Batch 713: Loss: 2.484375\n",
      "Batch 714: Loss: 4.3125\n",
      "Batch 715: Loss: 1.8046875\n",
      "Batch 716: Loss: 2.46875\n",
      "Batch 717: Loss: 1.890625\n",
      "Batch 718: Loss: 2.15625\n",
      "Batch 719: Loss: 5.28125\n",
      "Batch 720: Loss: 3.8125\n",
      "Batch 721: Loss: 2.5\n",
      "Batch 722: Loss: 4.21875\n",
      "Batch 723: Loss: 2.90625\n",
      "Batch 724: Loss: 4.1875\n",
      "Batch 725: Loss: 4.125\n",
      "Batch 726: Loss: 2.375\n",
      "Batch 727: Loss: 1.7734375\n",
      "Batch 728: Loss: 3.125\n",
      "Batch 729: Loss: 5.0\n",
      "Batch 730: Loss: 2.6875\n",
      "Batch 731: Loss: 3.015625\n",
      "Batch 732: Loss: 3.796875\n",
      "Batch 733: Loss: 3.234375\n",
      "Batch 734: Loss: 2.734375\n",
      "Batch 735: Loss: 4.46875\n",
      "Batch 736: Loss: 4.03125\n",
      "Batch 737: Loss: 3.15625\n",
      "Batch 738: Loss: 3.375\n",
      "Batch 739: Loss: 5.125\n",
      "Batch 740: Loss: 4.0625\n",
      "Batch 741: Loss: 4.09375\n",
      "Batch 742: Loss: 3.921875\n",
      "Batch 743: Loss: 3.5625\n",
      "Batch 744: Loss: 4.1875\n",
      "Batch 745: Loss: 2.6875\n",
      "Batch 746: Loss: 3.15625\n",
      "Batch 747: Loss: 2.53125\n",
      "Batch 748: Loss: 4.8125\n",
      "Batch 749: Loss: 2.953125\n",
      "Batch 750: Loss: 5.09375\n",
      "Batch 751: Loss: 2.171875\n",
      "Batch 752: Loss: 4.03125\n",
      "Batch 753: Loss: 1.640625\n",
      "Batch 754: Loss: 4.3125\n",
      "Batch 755: Loss: 2.671875\n",
      "Batch 756: Loss: 4.1875\n",
      "Batch 757: Loss: 3.9375\n",
      "Batch 758: Loss: 4.65625\n",
      "Batch 759: Loss: 4.375\n",
      "Batch 760: Loss: 3.6875\n",
      "Batch 761: Loss: 2.671875\n",
      "Batch 762: Loss: 4.1875\n",
      "Batch 763: Loss: 3.125\n",
      "Batch 764: Loss: 4.21875\n",
      "Batch 765: Loss: 4.40625\n",
      "Batch 766: Loss: 2.140625\n",
      "Batch 767: Loss: 4.03125\n",
      "Batch 768: Loss: 4.84375\n",
      "Batch 769: Loss: 2.796875\n",
      "Batch 770: Loss: 2.75\n",
      "Batch 771: Loss: 4.25\n",
      "Batch 772: Loss: 4.0625\n",
      "Batch 773: Loss: 3.375\n",
      "Batch 774: Loss: 3.03125\n",
      "Batch 775: Loss: 2.984375\n",
      "Batch 776: Loss: 2.625\n",
      "Batch 777: Loss: 4.875\n",
      "Batch 778: Loss: 3.609375\n",
      "Batch 779: Loss: 3.21875\n",
      "Batch 780: Loss: 3.859375\n",
      "Batch 781: Loss: 0.74609375\n",
      "Batch 782: Loss: 3.921875\n",
      "Batch 783: Loss: 4.15625\n",
      "Batch 784: Loss: 4.9375\n",
      "Batch 785: Loss: 3.171875\n",
      "Batch 786: Loss: 1.234375\n",
      "Batch 787: Loss: 3.828125\n",
      "Batch 788: Loss: 2.453125\n",
      "Batch 789: Loss: 3.015625\n",
      "Batch 790: Loss: 3.140625\n",
      "Batch 791: Loss: 3.734375\n",
      "Batch 792: Loss: 4.4375\n",
      "Batch 793: Loss: 2.890625\n",
      "Batch 794: Loss: 2.765625\n",
      "Batch 795: Loss: 3.421875\n",
      "Batch 796: Loss: 4.09375\n",
      "Batch 797: Loss: 3.796875\n",
      "Batch 798: Loss: 4.375\n",
      "Batch 799: Loss: 4.34375\n",
      "Batch 800: Loss: 4.28125\n",
      "Batch 801: Loss: 1.9296875\n",
      "Batch 802: Loss: 2.421875\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_train_epochs):\n",
    "    peft_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    print(epoch)\n",
    "    i = 0\n",
    "    for batch in train_data:\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if k != \"token_type_ids\" }\n",
    "        outputs = peft_model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        print(f\"Batch {i}: Loss: {loss.item()}\")\n",
    "        i += 1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_data)\n",
    "    avg_val_loss = evaluate(peft_model, validation_data)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_train_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCAcfn3lsmva"
   },
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"./hel_Finetune_en_hi_2\")\n",
    "tokenizer.save_pretrained(\"./hel_Finetune_en_hi_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZbaKH7wslPE"
   },
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLW1Qmp3tqC2"
   },
   "outputs": [],
   "source": [
    "input_text = \"today is a sunny day\"\n",
    "\n",
    "tokenized = tokenizer(\n",
    "    [input_text],\n",
    "    return_tensors = 'pt'\n",
    ").to(device)\n",
    "out = peft_model.generate(**tokenized, max_length = 128)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVTc7ZWJt_ov"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
