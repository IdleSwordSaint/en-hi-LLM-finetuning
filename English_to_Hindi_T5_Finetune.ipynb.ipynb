{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUXR5VZjp9Na",
    "outputId": "527de6f2-d2ed-4abe-c9cd-be3afbd928f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCo5Z9TO2FnX",
    "outputId": "c5874862-c9b0-4875-cf25-87daf77ce339"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.50.3)\n",
      "Requirement already satisfied: sacrebleu in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: peft in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.15.1)\n",
      "Requirement already satisfied: loralib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: rouge_score in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: evaluate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: portalocker in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu) (3.1.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu) (5.3.1)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (2.2.1+cu121)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (1.5.2)\n",
      "Requirement already satisfied: absl-py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: click in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers sacrebleu peft loralib rouge_score evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZUGAcJ9V2Lim"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YFRMnDUO2wUE"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"google-t5/t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F4I70BmO229x",
    "outputId": "ffe88cbd-cef8-4e7b-a74d-65bc3e7896c0"
   },
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"cfilt/iitb-english-hindi\") # Returns a DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MjMEppQA2_Fc",
    "outputId": "4f5b7042-326f-43dd-aa0d-a91655948976"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1659083\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 520\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2507\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYVg87xZ3MLg",
    "outputId": "bc6dc800-b2f3-49b1-e9f3-3f37e2d2735b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'Accerciser Accessibility Explorer',\n",
       "  'hi': 'एक्सेर्साइसर पहुंचनीयता अन्वेषक'}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3J3b1cn33eyF"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2h2TFml63j-Z"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f06140341714baa8082da3f18f4dee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0040070c9a554e70bf0ffd7b07681cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0596f25064ec40df9278985f622b6d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtaining the tokenizer designed specifically for the encoder the model, instead of using Word2Vec or TFID.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Not26eZ635St",
    "outputId": "da1c0cd9-f7a1-4750-ada5-2652fce18649"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [8774, 6, 48, 19, 3, 9, 7142, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text = \"Hello, this is a sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BcJ0FON5APX",
    "outputId": "8d54e736-2f49-43aa-c1a5-3d9c24d5ac70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[8774, 6, 48, 19, 3, 9, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text = [\"Hello, this is a sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v6POJRyM5Y28",
    "outputId": "fd8eb639-8532-4272-e4fc-1b568ad24aec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[8774, 6, 48, 19, 3, 9, 7142, 55, 1], [3, 2, 3, 2, 3, 2, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize using the Decoder specific toeknization rules.\n",
    "print(tokenizer(text_target = [\"Hello, this is a sentence!\", \"एक्सेर्साइसर पहुंचनीयता अन्वेषक\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WKwIM-y7GWU"
   },
   "source": [
    "#### We can see that the encoder and the decoder use the same tokenization logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HS8a4tMd7Nzi"
   },
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "source_lang = 'en'\n",
    "target_lang = 'hi'\n",
    "\n",
    "def preprocess_function(examples):\n",
    "  inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
    "  targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "  model_inputs = tokenizer(inputs, max_length = max_input_length, truncation = True)\n",
    "\n",
    "  # Setup the tokenization for targets\n",
    "  labels = tokenizer(text_target=targets, max_length = max_target_length, truncation = True)\n",
    "\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4X60z95Hhio",
    "outputId": "5670462b-6049-46f9-ef3b-2c7433ee6549"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[6434, 39, 917, 46, 17275, 7203, 1], [3, 19543, 15, 21645, 49, 5164, 11102, 15762, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 1], [3, 2, 3, 2, 3, 2, 1]]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(raw_datasets['train'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HfMB6nqaIb6b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b890b51386a04be8a9acc8f185bf1abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1659083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f830ed887064b84acaf4828e12c4f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2854b6d032403ea31022de7e022897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched = True) # Applies the function 'preprocess_function' to each split (train, test, val)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(raw_datasets[\"train\"].column_names) # Removes the columns that are not needed anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GULdHm2cIjoe",
    "outputId": "cb034f6a-b118-4b79-e6ad-ae8f63ae204d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 60506624\n",
      "all model parameters: 60506624\n",
      "percentage of trainable model parameters: 100.00%\n",
      "trainable model parameters: 589824\n",
      "all model parameters: 61096448\n",
      "percentage of trainable model parameters: 0.97%\n"
     ]
    }
   ],
   "source": [
    "# Selection of the model architecture, using the weights from the pretrained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, torch_dtype = torch.bfloat16).to(device) # Sequence-to-Sequence is used for Translation Language Modelling\n",
    "\n",
    "print(print_model_parameters(model))\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 16,\n",
    "    lora_alpha = 16,\n",
    "    target_modules = [\"q\", \"v\"],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config).to(device)\n",
    "print(print_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Data Collator will take the data in batches rather than the whole to pass it to the model\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer = tokenizer,\n",
    "    model = peft_model,\n",
    "    return_tensors = \"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "S34XXqzwODTC"
   },
   "outputs": [],
   "source": [
    "generation_data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer = tokenizer,\n",
    "    model = peft_model,\n",
    "    return_tensors = \"pt\",\n",
    "    pad_to_multiple_of = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "pBKAYLEZMiwj"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 2e-3\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "cLG-l-XEOV3O"
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(\n",
    "    tokenized_datasets['test'],\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    collate_fn = data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "K_Ek1wqdO4ua"
   },
   "outputs": [],
   "source": [
    "validation_data = DataLoader(\n",
    "    tokenized_datasets['validation'],\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    collate_fn = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "mr3y4YpwPXIi"
   },
   "outputs": [],
   "source": [
    "generation_data = DataLoader(\n",
    "    tokenized_datasets['validation'],\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    collate_fn = generation_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 1\n",
    "num_train_steps = num_train_epochs * len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "XogpNgZwcXGg"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(params = peft_model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Batch 0: Loss: 1.21875\n",
      "Batch 1: Loss: 0.7578125\n",
      "Batch 2: Loss: 0.72265625\n",
      "Batch 3: Loss: 0.48828125\n",
      "Batch 4: Loss: 0.5859375\n",
      "Batch 5: Loss: 0.546875\n",
      "Batch 6: Loss: 0.4296875\n",
      "Batch 7: Loss: 0.5859375\n",
      "Batch 8: Loss: 0.5625\n",
      "Batch 9: Loss: 0.328125\n",
      "Batch 10: Loss: 0.298828125\n",
      "Batch 11: Loss: 0.392578125\n",
      "Batch 12: Loss: 0.421875\n",
      "Batch 13: Loss: 0.326171875\n",
      "Batch 14: Loss: 0.318359375\n",
      "Batch 15: Loss: 0.36328125\n",
      "Batch 16: Loss: 0.44921875\n",
      "Batch 17: Loss: 0.4453125\n",
      "Batch 18: Loss: 0.3203125\n",
      "Batch 19: Loss: 0.478515625\n",
      "Batch 20: Loss: 0.365234375\n",
      "Batch 21: Loss: 0.412109375\n",
      "Batch 22: Loss: 0.3359375\n",
      "Batch 23: Loss: 0.40234375\n",
      "Batch 24: Loss: 0.40625\n",
      "Batch 25: Loss: 0.51953125\n",
      "Batch 26: Loss: 0.259765625\n",
      "Batch 27: Loss: 0.341796875\n",
      "Batch 28: Loss: 0.279296875\n",
      "Batch 29: Loss: 0.2060546875\n",
      "Batch 30: Loss: 0.640625\n",
      "Batch 31: Loss: 0.3125\n",
      "Batch 32: Loss: 0.34375\n",
      "Batch 33: Loss: 0.359375\n",
      "Batch 34: Loss: 0.2734375\n",
      "Batch 35: Loss: 0.32421875\n",
      "Batch 36: Loss: 0.34765625\n",
      "Batch 37: Loss: 0.322265625\n",
      "Batch 38: Loss: 0.31640625\n",
      "Batch 39: Loss: 0.2216796875\n",
      "Batch 40: Loss: 0.306640625\n",
      "Batch 41: Loss: 0.353515625\n",
      "Batch 42: Loss: 0.337890625\n",
      "Batch 43: Loss: 0.193359375\n",
      "Batch 44: Loss: 0.294921875\n",
      "Batch 45: Loss: 0.255859375\n",
      "Batch 46: Loss: 0.26953125\n",
      "Batch 47: Loss: 0.2578125\n",
      "Batch 48: Loss: 0.3046875\n",
      "Batch 49: Loss: 0.369140625\n",
      "Batch 50: Loss: 0.41796875\n",
      "Batch 51: Loss: 0.34375\n",
      "Batch 52: Loss: 0.251953125\n",
      "Batch 53: Loss: 0.232421875\n",
      "Batch 54: Loss: 0.2177734375\n",
      "Batch 55: Loss: 0.3671875\n",
      "Batch 56: Loss: 0.244140625\n",
      "Batch 57: Loss: 0.2421875\n",
      "Batch 58: Loss: 0.302734375\n",
      "Batch 59: Loss: 0.296875\n",
      "Batch 60: Loss: 0.328125\n",
      "Batch 61: Loss: 0.2490234375\n",
      "Batch 62: Loss: 0.34765625\n",
      "Batch 63: Loss: 0.36328125\n",
      "Batch 64: Loss: 0.29296875\n",
      "Batch 65: Loss: 0.275390625\n",
      "Batch 66: Loss: 0.283203125\n",
      "Batch 67: Loss: 0.28125\n",
      "Batch 68: Loss: 0.208984375\n",
      "Batch 69: Loss: 0.291015625\n",
      "Batch 70: Loss: 0.39453125\n",
      "Batch 71: Loss: 0.29296875\n",
      "Batch 72: Loss: 0.283203125\n",
      "Batch 73: Loss: 0.2294921875\n",
      "Batch 74: Loss: 0.2890625\n",
      "Batch 75: Loss: 0.328125\n",
      "Batch 76: Loss: 0.271484375\n",
      "Batch 77: Loss: 0.2734375\n",
      "Batch 78: Loss: 0.279296875\n",
      "Batch 79: Loss: 0.287109375\n",
      "Batch 80: Loss: 0.2275390625\n",
      "Batch 81: Loss: 0.43359375\n",
      "Batch 82: Loss: 0.2216796875\n",
      "Batch 83: Loss: 0.228515625\n",
      "Batch 84: Loss: 0.2138671875\n",
      "Batch 85: Loss: 0.35546875\n",
      "Batch 86: Loss: 0.275390625\n",
      "Batch 87: Loss: 0.328125\n",
      "Batch 88: Loss: 0.3671875\n",
      "Batch 89: Loss: 0.2412109375\n",
      "Batch 90: Loss: 0.2890625\n",
      "Batch 91: Loss: 0.333984375\n",
      "Batch 92: Loss: 0.259765625\n",
      "Batch 93: Loss: 0.40234375\n",
      "Batch 94: Loss: 0.2265625\n",
      "Batch 95: Loss: 0.373046875\n",
      "Batch 96: Loss: 0.33203125\n",
      "Batch 97: Loss: 0.41015625\n",
      "Batch 98: Loss: 0.265625\n",
      "Batch 99: Loss: 0.3125\n",
      "Batch 100: Loss: 0.3359375\n",
      "Batch 101: Loss: 0.2578125\n",
      "Batch 102: Loss: 0.193359375\n",
      "Batch 103: Loss: 0.3203125\n",
      "Batch 104: Loss: 0.1953125\n",
      "Batch 105: Loss: 0.224609375\n",
      "Batch 106: Loss: 0.291015625\n",
      "Batch 107: Loss: 0.408203125\n",
      "Batch 108: Loss: 0.275390625\n",
      "Batch 109: Loss: 0.416015625\n",
      "Batch 110: Loss: 0.375\n",
      "Batch 111: Loss: 0.275390625\n",
      "Batch 112: Loss: 0.298828125\n",
      "Batch 113: Loss: 0.341796875\n",
      "Batch 114: Loss: 0.259765625\n",
      "Batch 115: Loss: 0.2314453125\n",
      "Batch 116: Loss: 0.2890625\n",
      "Batch 117: Loss: 0.240234375\n",
      "Batch 118: Loss: 0.3984375\n",
      "Batch 119: Loss: 0.2734375\n",
      "Batch 120: Loss: 0.310546875\n",
      "Batch 121: Loss: 0.275390625\n",
      "Batch 122: Loss: 0.32421875\n",
      "Batch 123: Loss: 0.181640625\n",
      "Batch 124: Loss: 0.216796875\n",
      "Batch 125: Loss: 0.283203125\n",
      "Batch 126: Loss: 0.2890625\n",
      "Batch 127: Loss: 0.322265625\n",
      "Batch 128: Loss: 0.330078125\n",
      "Batch 129: Loss: 0.283203125\n",
      "Batch 130: Loss: 0.1875\n",
      "Batch 131: Loss: 0.30859375\n",
      "Batch 132: Loss: 0.375\n",
      "Batch 133: Loss: 0.2890625\n",
      "Batch 134: Loss: 0.2138671875\n",
      "Batch 135: Loss: 0.228515625\n",
      "Batch 136: Loss: 0.345703125\n",
      "Batch 137: Loss: 0.27734375\n",
      "Batch 138: Loss: 0.193359375\n",
      "Batch 139: Loss: 0.3046875\n",
      "Batch 140: Loss: 0.423828125\n",
      "Batch 141: Loss: 0.255859375\n",
      "Batch 142: Loss: 0.2578125\n",
      "Batch 143: Loss: 0.251953125\n",
      "Batch 144: Loss: 0.30078125\n",
      "Batch 145: Loss: 0.173828125\n",
      "Batch 146: Loss: 0.2412109375\n",
      "Batch 147: Loss: 0.31640625\n",
      "Batch 148: Loss: 0.263671875\n",
      "Batch 149: Loss: 0.2197265625\n",
      "Batch 150: Loss: 0.2431640625\n",
      "Batch 151: Loss: 0.2109375\n",
      "Batch 152: Loss: 0.2275390625\n",
      "Batch 153: Loss: 0.1796875\n",
      "Batch 154: Loss: 0.224609375\n",
      "Batch 155: Loss: 0.37890625\n",
      "Batch 156: Loss: 0.375\n",
      "Batch 157: Loss: 0.30859375\n",
      "Batch 158: Loss: 0.2060546875\n",
      "Batch 159: Loss: 0.24609375\n",
      "Batch 160: Loss: 0.171875\n",
      "Batch 161: Loss: 0.279296875\n",
      "Batch 162: Loss: 0.2109375\n",
      "Batch 163: Loss: 0.2109375\n",
      "Batch 164: Loss: 0.1904296875\n",
      "Batch 165: Loss: 0.333984375\n",
      "Batch 166: Loss: 0.240234375\n",
      "Batch 167: Loss: 0.306640625\n",
      "Batch 168: Loss: 0.32421875\n",
      "Batch 169: Loss: 0.3671875\n",
      "Batch 170: Loss: 0.1796875\n",
      "Batch 171: Loss: 0.2890625\n",
      "Batch 172: Loss: 0.294921875\n",
      "Batch 173: Loss: 0.1826171875\n",
      "Batch 174: Loss: 0.2060546875\n",
      "Batch 175: Loss: 0.1943359375\n",
      "Batch 176: Loss: 0.29296875\n",
      "Batch 177: Loss: 0.3125\n",
      "Batch 178: Loss: 0.240234375\n",
      "Batch 179: Loss: 0.29296875\n",
      "Batch 180: Loss: 0.287109375\n",
      "Batch 181: Loss: 0.16015625\n",
      "Batch 182: Loss: 0.2490234375\n",
      "Batch 183: Loss: 0.283203125\n",
      "Batch 184: Loss: 0.2734375\n",
      "Batch 185: Loss: 0.296875\n",
      "Batch 186: Loss: 0.2119140625\n",
      "Batch 187: Loss: 0.2099609375\n",
      "Batch 188: Loss: 0.251953125\n",
      "Batch 189: Loss: 0.279296875\n",
      "Batch 190: Loss: 0.298828125\n",
      "Batch 191: Loss: 0.216796875\n",
      "Batch 192: Loss: 0.2578125\n",
      "Batch 193: Loss: 0.28515625\n",
      "Batch 194: Loss: 0.2099609375\n",
      "Batch 195: Loss: 0.287109375\n",
      "Batch 196: Loss: 0.265625\n",
      "Batch 197: Loss: 0.359375\n",
      "Batch 198: Loss: 0.27734375\n",
      "Batch 199: Loss: 0.21484375\n",
      "Batch 200: Loss: 0.27734375\n",
      "Batch 201: Loss: 0.2119140625\n",
      "Batch 202: Loss: 0.2314453125\n",
      "Batch 203: Loss: 0.16796875\n",
      "Batch 204: Loss: 0.3125\n",
      "Batch 205: Loss: 0.24609375\n",
      "Batch 206: Loss: 0.349609375\n",
      "Batch 207: Loss: 0.314453125\n",
      "Batch 208: Loss: 0.23828125\n",
      "Batch 209: Loss: 0.2275390625\n",
      "Batch 210: Loss: 0.1357421875\n",
      "Batch 211: Loss: 0.275390625\n",
      "Batch 212: Loss: 0.236328125\n",
      "Batch 213: Loss: 0.25\n",
      "Batch 214: Loss: 0.2138671875\n",
      "Batch 215: Loss: 0.232421875\n",
      "Batch 216: Loss: 0.26171875\n",
      "Batch 217: Loss: 0.291015625\n",
      "Batch 218: Loss: 0.26171875\n",
      "Batch 219: Loss: 0.2421875\n",
      "Batch 220: Loss: 0.279296875\n",
      "Batch 221: Loss: 0.2353515625\n",
      "Batch 222: Loss: 0.201171875\n",
      "Batch 223: Loss: 0.298828125\n",
      "Batch 224: Loss: 0.240234375\n",
      "Batch 225: Loss: 0.25390625\n",
      "Batch 226: Loss: 0.431640625\n",
      "Batch 227: Loss: 0.1640625\n",
      "Batch 228: Loss: 0.197265625\n",
      "Batch 229: Loss: 0.33203125\n",
      "Batch 230: Loss: 0.1826171875\n",
      "Batch 231: Loss: 0.322265625\n",
      "Batch 232: Loss: 0.1796875\n",
      "Batch 233: Loss: 0.146484375\n",
      "Batch 234: Loss: 0.3046875\n",
      "Batch 235: Loss: 0.2451171875\n",
      "Batch 236: Loss: 0.228515625\n",
      "Batch 237: Loss: 0.2255859375\n",
      "Batch 238: Loss: 0.2119140625\n",
      "Batch 239: Loss: 0.220703125\n",
      "Batch 240: Loss: 0.224609375\n",
      "Batch 241: Loss: 0.26953125\n",
      "Batch 242: Loss: 0.255859375\n",
      "Batch 243: Loss: 0.201171875\n",
      "Batch 244: Loss: 0.30078125\n",
      "Batch 245: Loss: 0.62890625\n",
      "Batch 246: Loss: 0.34375\n",
      "Batch 247: Loss: 0.31640625\n",
      "Batch 248: Loss: 0.275390625\n",
      "Batch 249: Loss: 0.283203125\n",
      "Batch 250: Loss: 0.232421875\n",
      "Batch 251: Loss: 0.2021484375\n",
      "Batch 252: Loss: 0.3203125\n",
      "Batch 253: Loss: 0.23828125\n",
      "Batch 254: Loss: 0.28125\n",
      "Batch 255: Loss: 0.275390625\n",
      "Batch 256: Loss: 0.15625\n",
      "Batch 257: Loss: 0.1953125\n",
      "Batch 258: Loss: 0.3359375\n",
      "Batch 259: Loss: 0.251953125\n",
      "Batch 260: Loss: 0.1865234375\n",
      "Batch 261: Loss: 0.2333984375\n",
      "Batch 262: Loss: 0.2265625\n",
      "Batch 263: Loss: 0.1943359375\n",
      "Batch 264: Loss: 0.166015625\n",
      "Batch 265: Loss: 0.349609375\n",
      "Batch 266: Loss: 0.1982421875\n",
      "Batch 267: Loss: 0.244140625\n",
      "Batch 268: Loss: 0.224609375\n",
      "Batch 269: Loss: 0.2294921875\n",
      "Batch 270: Loss: 0.2890625\n",
      "Batch 271: Loss: 0.392578125\n",
      "Batch 272: Loss: 0.392578125\n",
      "Batch 273: Loss: 0.359375\n",
      "Batch 274: Loss: 0.2021484375\n",
      "Batch 275: Loss: 0.296875\n",
      "Batch 276: Loss: 0.263671875\n",
      "Batch 277: Loss: 0.251953125\n",
      "Batch 278: Loss: 0.2421875\n",
      "Batch 279: Loss: 0.267578125\n",
      "Batch 280: Loss: 0.259765625\n",
      "Batch 281: Loss: 0.298828125\n",
      "Batch 282: Loss: 0.255859375\n",
      "Batch 283: Loss: 0.1865234375\n",
      "Batch 284: Loss: 0.263671875\n",
      "Batch 285: Loss: 0.328125\n",
      "Batch 286: Loss: 0.28125\n",
      "Batch 287: Loss: 0.2431640625\n",
      "Batch 288: Loss: 0.2119140625\n",
      "Batch 289: Loss: 0.1298828125\n",
      "Batch 290: Loss: 0.287109375\n",
      "Batch 291: Loss: 0.30859375\n",
      "Batch 292: Loss: 0.2578125\n",
      "Batch 293: Loss: 0.2421875\n",
      "Batch 294: Loss: 0.173828125\n",
      "Batch 295: Loss: 0.267578125\n",
      "Batch 296: Loss: 0.1611328125\n",
      "Batch 297: Loss: 0.318359375\n",
      "Batch 298: Loss: 0.2578125\n",
      "Batch 299: Loss: 0.1533203125\n",
      "Batch 300: Loss: 0.220703125\n",
      "Batch 301: Loss: 0.212890625\n",
      "Batch 302: Loss: 0.2890625\n",
      "Batch 303: Loss: 0.244140625\n",
      "Batch 304: Loss: 0.255859375\n",
      "Batch 305: Loss: 0.20703125\n",
      "Batch 306: Loss: 0.2265625\n",
      "Batch 307: Loss: 0.177734375\n",
      "Batch 308: Loss: 0.25390625\n",
      "Batch 309: Loss: 0.2578125\n",
      "Batch 310: Loss: 0.1669921875\n",
      "Batch 311: Loss: 0.40234375\n",
      "Batch 312: Loss: 0.384765625\n",
      "Batch 313: Loss: 0.1708984375\n",
      "Epoch 1/1, Train Loss: 0.2896, Validation Loss: 0.1628\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_train_epochs):\n",
    "    peft_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    print(epoch)\n",
    "    i = 0\n",
    "    for batch in train_data:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        print(f\"Batch {i}: Loss: {loss.item()}\")\n",
    "        i += 1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_data)\n",
    "    avg_val_loss = evaluate(model, validation_data)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_train_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "LCAcfn3lsmva"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./T5-en-hi\\\\tokenizer_config.json',\n",
       " './T5-en-hi\\\\special_tokens_map.json',\n",
       " './T5-en-hi\\\\tokenizer.json')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.save_pretrained(\"./T5-en-hi\")\n",
    "tokenizer.save_pretrained(\"./T5-en-hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZbaKH7wslPE"
   },
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "SLW1Qmp3tqC2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3,\n",
      "         2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3,\n",
      "         2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 1]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hey! Tell be about transformers\"\n",
    "\n",
    "tokenized = tokenizer(\n",
    "    [input_text],\n",
    "    return_tensors = 'pt'\n",
    ").to(device)\n",
    "\n",
    "out = peft_model.generate(**tokenized, max_length = 128)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "kVTc7ZWJt_ov"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk></s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out[0]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
