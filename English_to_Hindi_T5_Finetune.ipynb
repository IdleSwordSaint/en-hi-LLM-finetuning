{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUXR5VZjp9Na",
    "outputId": "527de6f2-d2ed-4abe-c9cd-be3afbd928f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 26 01:30:42 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 571.96                 Driver Version: 571.96         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   57C    P0              9W /   66W |     617MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            8912    C+G   ...s\\Win64\\EpicGamesLauncher.exe      N/A      |\n",
      "|    0   N/A  N/A           13932    C+G   ...pp-4.0.433\\RazerAppEngine.exe      N/A      |\n",
      "|    0   N/A  N/A           14168    C+G   ...aries\\Win64\\EpicWebHelper.exe      N/A      |\n",
      "|    0   N/A  N/A           14508    C+G   ...crosoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A           39620    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           40292    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A           41336    C+G   ....0.3124.83\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           47216    C+G   ....0.3124.83\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           53652    C+G   ...64__zpdnekdrzrea0\\Spotify.exe      N/A      |\n",
      "|    0   N/A  N/A           66488    C+G   ...em32\\ApplicationFrameHost.exe      N/A      |\n",
      "|    0   N/A  N/A           68376    C+G   ...26wp6bftszj\\TranslucentTB.exe      N/A      |\n",
      "|    0   N/A  N/A           72224    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           96276    C+G   ...0_x64__8j3eq9eme6ctt\\IGCC.exe      N/A      |\n",
      "|    0   N/A  N/A           97740    C+G   ...l\\Programs\\Opera GX\\opera.exe      N/A      |\n",
      "|    0   N/A  N/A          119856    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCo5Z9TO2FnX",
    "outputId": "c5874862-c9b0-4875-cf25-87daf77ce339"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: transformers 4.50.1 does not provide the extra 'sentence-piece'\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers sacrebleu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "ZUGAcJ9V2Lim"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YFRMnDUO2wUE"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"google-t5/t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F4I70BmO229x",
    "outputId": "ffe88cbd-cef8-4e7b-a74d-65bc3e7896c0"
   },
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"cfilt/iitb-english-hindi\") # Returns a DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MjMEppQA2_Fc",
    "outputId": "4f5b7042-326f-43dd-aa0d-a91655948976"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1659083\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 520\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2507\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYVg87xZ3MLg",
    "outputId": "bc6dc800-b2f3-49b1-e9f3-3f37e2d2735b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'Accerciser Accessibility Explorer',\n",
       "  'hi': 'एक्सेर्साइसर पहुंचनीयता अन्वेषक'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3J3b1cn33eyF"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2h2TFml63j-Z"
   },
   "outputs": [],
   "source": [
    "# Obtaining the tokenizer designed specifically for the encoder the model, instead of using Word2Vec or TFID.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Not26eZ635St",
    "outputId": "da1c0cd9-f7a1-4750-ada5-2652fce18649"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [8774, 6, 48, 19, 3, 9, 7142, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text = \"Hello, this is a sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BcJ0FON5APX",
    "outputId": "8d54e736-2f49-43aa-c1a5-3d9c24d5ac70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[8774, 6, 48, 19, 3, 9, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text = [\"Hello, this is a sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v6POJRyM5Y28",
    "outputId": "fd8eb639-8532-4272-e4fc-1b568ad24aec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[8774, 6, 48, 19, 3, 9, 7142, 55, 1], [3, 2, 3, 2, 3, 2, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize using the Decoder specific toeknization rules.\n",
    "print(tokenizer(text_target = [\"Hello, this is a sentence!\", \"एक्सेर्साइसर पहुंचनीयता अन्वेषक\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WKwIM-y7GWU"
   },
   "source": [
    "#### We can see that the encoder and the decoder use the same tokenization logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "HS8a4tMd7Nzi"
   },
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "source_lang = 'en'\n",
    "target_lang = 'hi'\n",
    "\n",
    "def preprocess_function(examples):\n",
    "  inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
    "  targets = [ex[source_lang] for ex in examples[\"translation\"]]\n",
    "  model_inputs = tokenizer(inputs, max_length = max_input_length, truncation = True)\n",
    "\n",
    "  # Setup the tokenization for targets\n",
    "  labels = tokenizer(text_target=targets, max_length = max_target_length, truncation = True)\n",
    "\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4X60z95Hhio",
    "outputId": "5670462b-6049-46f9-ef3b-2c7433ee6549"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[6434, 39, 917, 46, 17275, 7203, 1], [3, 19543, 15, 21645, 49, 5164, 11102, 15762, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[6434, 39, 917, 46, 17275, 7203, 1], [3, 19543, 15, 21645, 49, 5164, 11102, 15762, 1]]}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(raw_datasets['train'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "HfMB6nqaIb6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1659083 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1659083/1659083 [03:21<00:00, 8252.94 examples/s] \n",
      "Map: 100%|██████████| 520/520 [00:00<00:00, 8457.21 examples/s]\n",
      "Map: 100%|██████████| 2507/2507 [00:00<00:00, 9714.20 examples/s] \n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched = True) # Applies the function 'preprocess_function' to each split (train, test, val)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(raw_datasets[\"train\"].column_names) # Removes the columns that are not needed anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GULdHm2cIjoe",
    "outputId": "cb034f6a-b118-4b79-e6ad-ae8f63ae204d"
   },
   "outputs": [],
   "source": [
    "# Selection of the model architecture, using the weights from the pretrained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device) # Sequence-to-Sequence is used for Translation Language Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Data Collator will take the data in batches rather than the whole to pass it to the model\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer = tokenizer,\n",
    "    model = model,\n",
    "    return_tensors = \"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "S34XXqzwODTC"
   },
   "outputs": [],
   "source": [
    "generation_data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer = tokenizer,\n",
    "    model = model,\n",
    "    return_tensors = \"pt\",\n",
    "    pad_to_multiple_of = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "pBKAYLEZMiwj"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 2e-15\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "cLG-l-XEOV3O"
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(\n",
    "    tokenized_datasets['test'],\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    collate_fn = data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "K_Ek1wqdO4ua"
   },
   "outputs": [],
   "source": [
    "validation_data = DataLoader(\n",
    "    tokenized_datasets['validation'],\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    collate_fn = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "mr3y4YpwPXIi"
   },
   "outputs": [],
   "source": [
    "generation_data = DataLoader(\n",
    "    tokenized_datasets['validation'],\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    collate_fn = generation_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 5\n",
    "num_train_steps = num_train_epochs * len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "XogpNgZwcXGg"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(params = model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[185], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     12\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Projects\\Gen AI\\Assn-1\\venv\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\Gen AI\\Assn-1\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\Gen AI\\Assn-1\\venv\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_train_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    print(epoch)\n",
    "    i = 0\n",
    "    for batch in train_data:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        print(f\"Batch {i}: Loss: {loss.item()}\")\n",
    "        i += 1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_data)\n",
    "    avg_val_loss = evaluate(model, validation_data)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_train_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCAcfn3lsmva"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./T5_Finetune_en_hi\")\n",
    "tokenizer.save_pretrained(\"./T5_Finetune_en_hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZbaKH7wslPE"
   },
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLW1Qmp3tqC2"
   },
   "outputs": [],
   "source": [
    "input_text = \"Hey! Tell be about transformers\"\n",
    "\n",
    "tokenized = tokenizer(\n",
    "    [input_text],\n",
    "    return_tensors = 'np'\n",
    ")\n",
    "out = model.generate(**tokenized, max_length = 128)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVTc7ZWJt_ov"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(out[0], skip_special_tokens = True))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
